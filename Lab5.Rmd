---
title: "Lab5"
author: "Mariam Garcia"
date: "2024-05-08"
output: html_document
---

### Lab 5 Assignment

#### Train Your Own Embeddings

1.  Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi"

    ```{r warning = FALSE, message = FALSE, include = FALSE }
    library(tidytext)
    library(tidyverse)
    library(widyr)
    library(irlba) 
    library(broom) 
    library(textdata)
    library(ggplot2)
    library(dplyr)
    library(LexisNexisTools)

    nexis_files <- list.files(('./nexis-files/'), pattern = '.docx', full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

    # reading in the files with lnt_read

    data <- lnt_read(nexis_files)


    metadata <- data@meta

    articles <- data@articles

    paragraphs <- data@paragraphs

    data_tib <- tibble(Date = metadata$Date, Headline = metadata$Headline, id = articles$ID, text = articles$Article)

    # Assuming data_tib is your tibble
    write.csv(data_tib, "lexis_files.csv", row.names = FALSE)


    # reading in csv 

    lexis_files_csv <- read_csv('lexis_files.csv')


    # calculating unigram probabilities ------------------------------------
    unigram_prob <- lexis_files_csv %>% 
      unnest_tokens(word,text) %>% 
      anti_join(stop_words, by = 'word') %>% #stop here and let them take a stab
      count(word, sort=T) %>%
      mutate(p=n/sum(n))

    # calculating skipgram probabilities ---------------

    skipgrams <- lexis_files_csv %>% 
      unnest_tokens(ngram,text, token = 'ngrams', n =5) %>% 
      mutate(ngramID = row_number()) %>% 
      tidyr::unite(skipgramID, id, ngramID) %>% 
      unnest_tokens(word,ngram) %>% 
      anti_join(stop_words, by = 'word')


    # ------ sum total # of occurrences of each pair of words

    skipgram_probs <- skipgrams %>% 
      pairwise_count(item = word,  feature = skipgramID, diag = F,sort = T, upper = F) %>% 
      mutate(p = n/sum(n))

    # ------ normalizing probabilities

    normalized_probs <- skipgram_probs %>% 
      rename(word1 = item1, word2 = item2) %>% 
      left_join(unigram_prob %>% 
                  select(word1 = word, p1=p),
                by = 'word1') %>% 
      left_join(unigram_prob %>% 
                  select(word2 = word, p2 = p),
                by = 'word2') %>% 
      mutate(p_together = p/p1/p2)


    # ------ calculating point-wise mutual information measure
    pmi_matrix <- normalized_probs %>% 
      mutate(pmi = log10(p_together)) %>% 
      cast_sparse(word1,word2,pmi)
    ```

2.  Think of 3 important words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.

    ```{r}
    # three most important words: cellular, telephone, safety
    # Perform SVD using irlba
    pmi_svd <- irlba(pmi_matrix, 100, verbose = F)

    # Extract left singular vectors
    word_vectors <- pmi_svd$u

    # Assign row names to the word vectors for ease of reference
    rownames(word_vectors) <- rownames(pmi_matrix)

    ```

3.  Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.

#### Pretrained Embeddings

4.  Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings. These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

Note: The embeddings .zip file is very large. You may have to increase your global timeout setting to download, ex: options(timeout=100)

5.  Test them out with the canonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?

Hint: you'll need to convert the GloVe dataframe to a matrix and set the row names in order to use our synonym function.

6.  Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you made. How do they compare? What are the implications for applications of these embeddings?
